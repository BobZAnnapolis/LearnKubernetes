UDEMY Learn Kubernetes From A DevOps Guru Notes
===============================================


Section 1
=========

course examples available at https://github.com/jleetutorial/kubernetes-demo

minikube - runs actually k8s code, best for testing out stuff on workstations

MINIKUBE
- all-in-one install of k8s
- takes all the distributed components of k8s and packages them into a single vm to run locally
- a few caveats :
  - mk does NOT support Cloud Provider specific features such as Load Balancers, Persistent Volumes
  - mk requires Virtualization - on Linux VirtualBox or VMWare Fusion

Terminology
K8S "deployments" are the high-level constructs that define an application
"pods" are instances of a container in a deployment
"services" are endpoints that expose ports to the outside world
You can delete, create, modify and retrieve information about any of these using the kubectl commands and local UI

Mac OS
- brew update
- brew install kubectl
  - needed to install Xcode command line tools
- brew install homebrew/cask/minikube
- brew cask install virtualbox

- minikube start
  kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
  kubectl expose deployment hello-minikube --type=NodePort
  kubectl get pod
  curl $(minikube service hello-minikube --url)
  kubectl delete deployment hello-minikube
  kubectl get pod
  minikube stop


What does a k8s app look like ?
- 'deployments' are the central metaphor for what we'd consider 'apps' or 'services'
- deployments are described as a collection of resources and references
- deployments take many forms based on the type of services being deployed
- typically described in YAML format

1st k8s app, tomcat
- deploy tomcat app server using official docker image
- tasks
  - define the deployment
    - most simple deployment is a single pod, no redundancy, no separation of services
    - a pod is an instance of a container
    - deployments can have any number of pods required to get the job done
  - expose its services
  - deploy it to our cluster

If a registry isn't specified - k8s will default to online docker hub

- minikube start
- define the deployment
  - kubectl apply -f kubernetes-demo/01_Introduction_to_Kubernetes/Your\ First\ k8s\ App/deployment.yaml
- map exposed service in container to outside world 
  - kubectl expose deployment tomcat-deployment --type=NodePort
- ask minikube what port it chose to expose
  - minikube service tomcat-deployment --url
- verify we could access the app at the given port
  - curl http://192.168.99.100:32047


KUBECTL
- basic command to interact w/any k8s cluster, local, remote, etc
- primary command line access tool
- kubectl get pods
  - list all running
- kubectl describe pods
  - detailed info on all
  - specify name to list just 1
  - lists LOTS of info

To have running containers communicate with outside world, have to expose a port 
  - kubectl expose 
    - exposes a port {UDP or TCP} for a given deployment, pod, or other resource
    - can specify a port or let k8s select one

kubectl port-forward
  - forwards 1 or more local ports to a pod
  - kubectl port-forward <pod name> [LOCAL_PORT:]REMOTE_PORT]

kubectl attach
  - attach to a pod to see its output
  - attaches to a process that is already running inside an existing container 

kubectl exec
  - execute a command inside a container
  - -i option will pass stdin to the container
  - -t option will specify stdin as TTY
  - eg, get to a bash shell to debug
    - kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 bash

kubectl label pods
  - updates labels on a pod for record-keeping, auditing purposes
  - eg, kubectl label pods tomcat-deployment-56ff5c79c5-vkrj9 healthy=false
    - "healthy=false" associated with this pod

kubectl run
  - instead of using a YAML deployment file
  - can also specify a command line image to run
  - runs the specified image on the cluster, specify name, port to expose, etc


KUBECTL REFERENCES & CHEAT SHEET
https://kubernetes.io/docs/user-guide/kubectl/v1.10
https://kubernetes.io/docs/user-guide/kubectl-cheatsheet

bash commands executed :
  543  xcode-select --install
  544  brew install kubectl
  545  kubectl
  551  kubectl version
  553  brew install homebrew/cask/minikube
  556  brew cask install virtualbox
  557  minikube start
  558  kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
  559  kubectl expose deployment hello-minikube --type=NodePort
  560  kubectl get pod
  561  curl $(minikube service hello-minikube --url)
  562  kubectl delete deployment hello-minikube
  563  kubectl get pod
  564  minikube stop
  565  history
  567  cat kubernetes-demo/01_Introduction_to_Kubernetes/Your\ First\ k8s\ App/deployment.yaml 
  569  minikube start
  570  kubectl apply -f kubernetes-demo/01_Introduction_to_Kubernetes/Your\ First\ k8s\ App/deployment.yaml 
  571  kubectl expose deployment tomcat-deployment --type=NodePort
  573  minikube service tomcat-deployment --url
  574  curl http://192.168.99.100:32047
  575  history
  577  kubectl get pods
  579  kubectl describe pods tomcat-deployment-56ff5c79c5-vkrj9
  580  kubectl get pods
  581* kubectl attach tomcat-deployment-56ff5c79c5-vkrj
  582  kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 bash
  587  kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 hostname
  588  kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 bash
  589  kubectl label pods tomcat-deployment-56ff5c79c5-vkrj9 healthy=false
  590  kubectl get pods
  591  kubectl run hazelcast --image=hazelcast --port=5701
  592  kubectl describe pods
  593  kubectl version
  594  history


================================================================================
================================================================================


Section 2 Architecture Overview
===============================

Masters and Nodes

developer/operator/sysadmin/engineer interacts w/master(s)

Masters 
  - API Server
  - Scheduler
  - Controller Manager  (cmgr)

cmgr interacts w/k8s node(s)

nodes
  - kubelet
  - kube-proxy
  - 1 or more pods
  - Docker

users access the applications running on the pod(s)

A node is a 'worker machine' in k8s, a vm or a physical machine
Has the services necessary to run pods

Each node has a common set of services, eg, Docker, kubelet, cube-proxy
kubelet - monitors/runs pods
kube-proxy - glue on each node that makes sure network services on each node can be accessed

How should applications be designed to take advantage of scaling
stateful versus stateless
how state is handled can either enable or interfere with scaling, it most certainly dictates how you'll scale

stateful applications create/save application-/session-specific data locally - essentially lock apps to 1 machine - not a good idea if you want/need scaling
if that server dies - that state is lost - it is also the only 1 that knows what state that user's data is in

stateless applications update a central data store, eg, db, when necessary and any server can handle any request at any time to continue processing

K8S provides many ways to implement replication/scaling in your deployment
YAML file has a 'replicas' keyword -most popular
- define a replicaset
- deploy multiple bare pods
- define jobs
- daemonset


If your original deployment is still running and did NOT specify REPLICAS > 1, 
to prevent any downtime, you can issue a kubectl command
- kubectl scale --replicas=4 deployment/tomcat-deployment
- or if it's stopped, when you re-deploy - modify the replicas keyword in the YAML file


you can run the get pods or get deployments to see that there are now 4 replicas runnings
how does k8s decide which one gets a deployment ?

the only way pods can be exposed to the outside world is thru defined services

we previously ran the "kubectl expose deployment tomcat-deployment --type=NodePort" cmd when there was just 1 instance running

the "Load Balancer" service provides this function

the load balancer exposes just a single port but it decides which replica to send the request to
kubectl expose deployment tomcat-deployment --type=LoadBalancer --port=8080 --target-port=8080 --name tomcat-load-balancer
kubectl describe services tomcat-load-balancer
Name:                     tomcat-load-balancer
Namespace:                default
Labels:                   app=tomcat
Annotations:              <none>
Selector:                 app=tomcat
Type:                     LoadBalancer
IP:                       10.108.164.206
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  31552/TCP
Endpoints:                172.17.0.3:8080,172.17.0.6:8080,172.17.0.7:8080 + 1 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


DEPLOYMENTS
===========
high level object in k8s
define a desired state of an application

consist of pods

optionally, they can also include replicasets - which will be automagically managed by the deployment based upon its replica cfg

with deployment objects, you can :
- create a newdeployment
- update an existing {eg, set number of replicas}
- apply rolling updates to pods running on your cluster
- rollback to a previous version
- pause & resume

kubectl is your gateway to working w/deployments
useful commands
list deployments
  - kubectl get deployments
view status of deployment rollouts
  - kubectl rollout status deployment deployment_nm
set the image of a deployment
  - kubectl set image
    - look in the deployment.yaml file - in the container section - can change the image in any container using
      this command as long as we know the name, image version and deployment name
      eg, use get deployments to get deployment name, kubectl set image deployment/tomcat-deployment tomcat=tomcat=tomcat:9.0.1; do jubectl describe deployments afterward to see new image version
view the history of a rollout, including previous revisions
  - kubectl rollout history deployment tomcat-deployment to see list of mods
  - add --revision=# to get description of the change


LABELS & SELECTORS
==================

as the cluster grows, there will be an increasing number of pods, resources, services, etc - keeping track of everything can get cumbersome 

a method to keep things organized and to hols you and k8s identify resources to act upon

labels are key/value pairs that you can attach to objects like pods
  - they are for users to help describe meaningful and relevant info about an object
  - they do not affect the semantics of the core system
selectors are a way of expressing how to select objects based upon their labels
  - simple language to define what labels match and which ones do
  - you can specify if a label equals a given criteria or if it fits inside a set of criteria
    - equality-based
    - set-based

you can label nearly anything in the k8s world
  - deployments
  - services
  - nodes

eg, use labels to label a nice that it has SSD storage and then use a selector to tell the deployment that our app should only ever go onto a node with SSD storage



HEALTH CHECKING
===============

what happens when something in our cluster goes wrong ?

k8s has 'health checks'
- readiness problems
- liveness problem

readiness probes : determine when a pod is "ready", eg, after it has started to see when it's ready and has loaded what it needs to internally in the image and is ready to take requests from external services

liveness probes : determine when a pod is 'healthy' or 'unhealthy' after it has become ready

whether they are a readiness or a liveness probe they can use a variety of methods to ascertain a container's status
  - successful HTP or TCP request to the pod
  - successful command execution on the pod
probes are defined on the container in a Deployment or Pod specification

on our tomcat example :
- a readiness probe will check to make sure the Pod has started and is ready to begin taking requests
- a liveness probe on the tomcat deployment will help us ensure the containers continue to be able to accept and service requests without error in a reasonable amount of time

  657  cd Labels\ \&\ Selectors/
  658  lsa
  660  cat deployment.yaml 
  661  kubectl describe deployments
  662  cat deployment.yaml 
  663  kubectl get nodes
  664  kubectl label node minikube storageType=ssd
  665  kubectl get nodes
  666  kubectl describe node minikube
       - see storage=ssd in labels section
  667  cat deployment.yaml 
  668  kubectl apply -f deployment.yaml 
  669  cd..
  670  cd Health\ Checks/
  671  lsa
  674  cat deployment.yaml 
  675  kubectl apply -f ./deployment.yaml 
  676  kubectl describe deployment tomcat-deployment
       - see new lines with readiness and liveness probes running


WEB INTERFACE
=============

command line kubectl commands are the main interface
also has a web interface
called "Dashboard UI"
runs on k8s master(s)
accessible directly if you have direct network connectivity directly to your cluster/master(s) - unlikely in production envs

kubectl can create a proxy/tunnel if you do not "kubectl proxy"

provides a variety of views for nearly all of the command line kubectl commands

"minikube dashboard" command bring up UI


EXERCISE
========
get latest mongoldb replicates {4} up and running on our single node
1 way
  - 2 commands
    - kubectl run mongo-exercise-1 --image=mongo --port=27017
    - kubectl scale --replicas=4 deployment/mongo-exercise-1

Other ways
  - write a deployment file, use kubectl apply & kubectl expose
  - write a deployment file and a service file and use kubectl apply on both
  - use a K8S package Manager like helm to handle the work for you {advanced}
  - Use the Dashboard UI

  703  cd 02_Basic_and_Core_Concepts/
  705  cd Scaling\ \&\ Replication/
  707  cat deployment.yaml 
  708  cp deployment.yaml mongodb.yaml 
  710  vim mongodb.yaml 
  714  kubectl apply -f mongodb.yaml 
  778  kubectl expose deployment mongodb-deployment --type=NodePort
  748  kubectl get pods
  777  kubectl describe deployment mongodb-deployment
  780  kubectl get deployments
  780  kubectl get deployments

================================================================================
================================================================================

Section 3
=========

DNS & Service Discovery
=======================

DNS - Domain Name Service, cornerstone of the Internet
translates names into IP addresses
K8S has a build-in DNS service that is launched & cfg'd automatically
K8S cfgs kubelets to tell individual containers to use the DNS service's IP to resolve DNS names

Service Discovery : Modern software best practices consist of having completely separate deployment, scaling and configuration steps to build modern systems. 

How do we get them to communicate with each other in order to create a viable useful application in a containable way ? Service Discovery

Every service in your K8S cluster gets a DNS name
K8S has a specific and consistent nomenclature for deciding what this DNS name is
- general form is <my-service-name>.<my-namespace>.svc.cluster.local
But, you can use this well-known pattern to design loosely-coupled services or at least set "sane defaults"
- 1 could use environment variables to set the DNS name of other services your app may need
- or you could expect services at set DNS names
- or combine the 2 above and expect services at well-known DNS names and allow environment variables to override


a note on namespaces
- we'll discuss namespaces in depth soon in their own lecture, but for now, know the following
- k8s allows you to define "namespaces" - they allow you to separate your cluster into smaller logical clusters
  - by default, everything you do is in the "default" namespace (and everything we've done so far has been)
  - k8s DNS names will include this namespace in the assigned DNS name
- namespaces are helpful mostly for large clusters with many users across many teams & projects
  - using the default namespace is fine if this logical separation is not required 
  - eg of when needed, small homework assignment :no; MySQL instances for large corporations - have namespaces for the different departments would help create logical separations

HOW APP01 COULD REACH APP02 USING DNS

POD01
  - has a container
  - running Application_1
  - has a SERVICE: APP1 w/ip: 10.0.0.1

POD02
  - has a container
  - running Application_2
  - has a SERVICE: APP2 w/ip: 10.0.0.2

$ HOST APP1-SERVICE
APP1-SERVICE HAS ADDRESS 10.0.0.1
$ HOST APP2-SERVICE
APP2-SERVICE HAS ADDRESS 10.0.0.2
$ HOST APP2-SERVICE.DEFAULT
APP2-SERVICE.DEFAULT HAS ADDRESS 10.0.0.2
$ HOST APP2-SERVICE.DEFAULT.SVC.CLUSTER.LOCAL
APP2-SERVICE.DEFAULT.SVC.CLUSTER.LOCAL HAS ADDRESS 10.0.0.2

When using K8S - it has a built-in kube-dns 'process' that handles dns resolution automatically and is configured to use the default cfg'd namespaces so as long as the cluster abides by that naming convention - the built-in kube-dns will work

  910  kubectl get deployments
  911  kubectl delete deployment/mongodb-deployment
  913  kubectl delete deployment/tomcat-deployment
  914  kubectl get deployments
  915  kubectl get pods
  919  vim ./mysql-deployment.yaml 
  920  kubectl create -f ./mysql-deployment.yaml 
  921  kubectl get pods
  922  lsa
  923  vim wordpress-deployment.yaml 
  924  kubectl create -f ./wordpress-deployment.yaml 
  925  kubectl get pods
  933  kubectl get services wordpress
  934  minikube service wordpress --url
  - copy-paste-url to access wordpress site


VOLUMES
=======

Normally, unless otherwise configured - data in a Docker container is ephemeral, ie, once container is shut down - data vanishes.

This is essentially a stateless implementation - it is at the core of Kubernetes and containerization.

Downside - what do you do if you need persistent data ? That's where volumes come in.

Kubernetes provides directives that users cause to "attach" their physical volumes to their logical containers.

Volumes are like disks, but with a bit more
- volumes can be considered just a directory, with some data, which containers in a pod can access
- K8S supports multiple types of volumes that take care of how that data is stored, persisted, and made available
  - Support for a variety of cloud providers' block store products
  - Support for SAN-type hardware, file systems, etc
  - Support for local volumes {for testing/minikube only ! Not production}
- Certain types of Volumes can also provide sharing of files between pods babying mounted to multiple Pods simultaneously

Types of Volumes
- this is an incomplete list {see K8S documentation for complete list}
- Cloud Provider
  - Azure Disk & Azure File
  - AWS EBS - Elastic Block Store
  - Google Compute Engine Persistent Disk
- SAN/File System/Hardware
  - CephFS
  - Fibre Channel
  - GlusterFS
  - NFS
  - iSCSI
  - Local (For development/minikube only - not supported in production)


USING VOLUMES
=============
Pods can specify what volumes they need and where to mount them
  - Using the spec.volumes field(what volumes they need)
  - Using the spec.containers.volumeMounts field (where to mount them)

Processes in the containers then see a filesystem view of the data in that Volume

Using Volumes lets us separate stateless portions of our application (the code) from stateful data
  - The infrastructure can be scaled, maintained, and live separately from the data it works on/with
  - Also may ease portability, backup, recovery, and other management tasks in well-architected systems

We don't need to backup the Docker containers cuz they live inside a repository
But we may need to backup the data - separating the pieces this way, ie code in Docker registry, data in Volumes - allows us a variety of options to keep a system backed up

Separating them out eases the management of backing an app up

USING PERSISTENT VOLUMES
========================
PersistentVolumes ate Volumes designed to provide persistent disk-like functionality, ie, data survives reboots
Using then involves :
  - Provisioning a PersistentVolume (akin to creating/installing a disk in a virtual machine or hardware server)
  - Establishing a PersistentVolumeClaim (it is a request for storage by a user/Pod)
By examining available PersistentVolumes and demands from PersistentVolumeClaims by running Pods, K8S binds available volumes to Pods based on the options specified by the PersistentVolumeClaims to matching PersistentVolumes, eg, by name, storage size, storage class, etc asked for in the Claims


HOW CLAIMS & VOLUMES INTERACT
=============================
Administrator will define Volumes available for use, based upon available options
Developer/User will add VolumeClaims to their pod deployment yams files
When the app starts - the claims are sent to the K8S Master who matches claims to the available defined volumes and does the attaching to the Pods at runtime

Sometimes, if possible, the K8S Master will allocate new volumes under certain circumstances, eg, load balancing....if it can - ie, dynamic provisioning


CREATING A VOLUME
=================
PersistentVolumes are defined using a "PersistentVolume" definition that specifies their type, size, and how they can be accessed
Their type and access type is highly dependent on the underlying media 
- local disk
- network mount
- Cloud Block Storage Service
- Directory on the Host (testing only, not for production)

YAML files have a set of keywords to use based upon access type, eg local

bobzawislak (master *) 02_Volumes $ cat local-volumes.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
  labels:
    type: local
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data/pv-1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-2
  labels:
    type: local
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data/pv-2

Based upon the "kind" - access K8S documentation for all the other parms


CLAIMING A VOLUME
=================
Pods use PersistentVolumeClaims to request physical storage defined by PersistentVolumes
K8S uses the Claim to look for a PersistentVolume that satisfies the claim's requirements
- if it finds a match, it binds the claim to the volume
- if it cannot find a match (& automatic provisioning is NOT enabled), it results in an error


videos walked thru creating the local volumes, applying the new YAML files to the running WP site,
getting the URL to the WP site, adding a post - if saved correctly, indicates that the information
was saved into the new volumes

delete the wordpress deployment  and try to reload - browser can't - kubectl get deployments lists just the wordpress-mysql deployment

recreate the wordpress deployment using existing file - ok to ignore some of the errors cuz we're previously created a few of them

once the deployment exists - get URL {should be the same} and re-load

 1086  kubectl create -f local-volumes.yaml
 1087  kubectl apply -f mysql-deployment.yaml 
 1088  kubectl apply -f wordpress-deployment.yaml 
 1089  kubectl get pods
 1092  kubectl describe pod wordpress-5cddd8c7b-vf2s4
 1093  kubectl describe pod wordpress-mysql-5577fbc78-fnq7z
 1095  cat wordpress-deployment.yaml 
 1096  minikube service wordpress --url
       - create the wp site
       - login
       - create a post
       - get to home page and see the post - this shows data was saved in the persistent volume db
 1097  kubectl get deployments
 1098  kubectl delete deployment word-press
       - this works - can no longer load the page
 1099  kubectl get deployments
 1100  kubectl create -f wordpress-deployment.yaml 
       - this says it works
       - get 2 errors due to previous services and persistentVolumeClaims being created - ignore
 1101  minikube service wordpress --url
       - should be same as before
       - refresh page - site should be displayed - and previous post should still exist

this is an example of how you can divide up functionality
The code originally had wordpress and mysql yaml files that had secret refs in them
    - had to replace those with the hard-coded passwords from the yamls from chapter 01 


03 SECRETS
==========

credentials to :
- access their systems
- access their systems securely
- access other systems

holds access information "secretly" and separately from plaintext items

Secrets contain small amounts of data
Secrets can be delivered to a pod in the form of :
- a file placed on a volume at runtime containing the secret data (useful for certificates)
- an environment variable referenced by the Pod and inserted at runtime into the environment by the kubelet running the Pod - just like any other environment variable

HOW SECRETS ARE STORED
======================
- K8S provides separation for secrets, it does NOT provide strong encryption
- secrets are typically Base64 encoded strings stored separately from configuration and injected at runtime
- you can encode it manually or use K8S' tools to do it for you

HOW SECRETS ARE STRUCTURED
==========================
- secrets are key:value pairs, both the key and the value are arbitrary strings

K8S uses Base64 encoding

CREATING A SECRET
=================
kubectl has commands
from a file
  - kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt
from a literal on the command line
  - kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD

once it is created, the 2nd part is using it
cause it as an environment variable

USING A SECRET AS AN ENVIRONMENT VARIABLE
=========================================
check the following section of the wordpress-mysql yaml file
       env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password

 1331  kubectl create secret generic mysql-pass --from-literal=password=k8s
 1332  kubectl get secret
 1333  kubectl get deployments
 1334  kubectl describe deployment wordpress-mysql
       - should see plaintext password
 1335  kubectl apply -f mysql-deployment.yaml
       kubectl describe deployment wordpress-mysql
       - should now see a password stored in the secret

 1336  kubectl describe deployment wordpress-mysql
       - should see plaintext password
 1337  kubectl apply -f wordpress-deployment.yaml 
 1338  kubectl describe deployment wordpress
       - should now see a password stored in the secret
       - it wasn't, was still stored as plaintext :-)
       - asked a question on the course forum
       - got the response that "sometimes" you have to do a "kubectl delete -f wordpress-deployment.yaml"
         and then the apply
       - i tried that, that didn't work
       - then i deleted both *-deployment.yamls ; made sure they were gone ; then apply'd both
       - this worked - YEAH !!!!


04 USAGE AND RESOURCE MONITORING
================================

now that the systems are running that can persist - how does it live in the wild ?

we want it to run reliably - we have to perform health checks, see how it is using resources, etc

need metrics and resource monitoring

MANY SOLUTIONS
==============
usage and resource monitoring
many free & paid packages and services both for K8S and beyond
we'll explore K8S' "native" stack and a few open source additions
  - Heapster
  - backed by InfluxDB
  - visualized by Grafana

these are installed on most Cloud providers by default, eg AWS GCE
When using MiniKube - have to enable them manually - see steps captured below

Hipster is a CNCF {??}
- installed as a cluster-wide pod, gathers monitoring and events data from each pod on each node by talking to each pods kubelet, kubelet fetches the data from cAdvisor, an internal K8S system; 
- Heapster then sends this data to InfluxDB
- use Grafana to visualize the data gathered - make decisions based upon needs

HEAPSTER
  - K8S Container Cluster Monitoring solution
  - maintained by CNCF
  - collects and interprets various signals like compute resource usage, lifecycle events, etc
  - using pluggable back-ends, we'll use InfluxDB
  - collects by default all data monitored by K8S built-in cAdvisor fun
    - additional source are described in the documentation
  
ENABLING HEAPSTER ON MINIKUBE
  - comes enabled on most cloud-based platforms
  - On MINIKUBE, we'll have to enable the add-on
    - minikube adding enable heapster
  - Grab a snack or a beer, it may take a few minutes or longer for Heapster to gather all the stats with . . . .
    - kubectl get pods --namespace=kube-system

bobzawislak (master *) 04_Usage_and_Resource_Monitoring $ kubectl get pods --namespace=kube-system
NAME                                    READY     STATUS    RESTARTS   AGE
etcd-minikube                           1/1       Running   0          23h
heapster-vblcg                          1/1       Running   0          1m
influxdb-grafana-q64dj                  2/2       Running   0          1m
kube-addon-manager-minikube             1/1       Running   0          1d
kube-apiserver-minikube                 1/1       Running   0          1d
kube-controller-manager-minikube        1/1       Running   0          1d
kube-dns-86f4d74b45-qc2f5               3/3       Running   0          1d
kube-proxy-clzjh                        1/1       Running   0          1d
kube-scheduler-minikube                 1/1       Running   0          1d
kubernetes-dashboard-5498ccf677-5p27m   1/1       Running   0          1d
storage-provisioner                     1/1       Running   0          1d

  - when pods are up access the Grafana Dashboard with
    - minikube addons open heapster 
      - upper left, sign in admin / admin
      - upper left, change namespaces and select different pod names to see kinds of information collected/displayed


NAMESPACES & RESOURCE QUOTAS {Lecture 29}
=========================================

K8S allows you to name objects of any type
works ok for single user clusters

for multiple teams each running their own version - namespaces

- namespaces create multiple virtual clusters on the same physical clusters
  - the virtual clusters are called namespaces
- namespaces provide scope and separation of clusters
  - namespaces need to be unique but the names of things w/in namespaces can be the same
    - this "could" be useful if similar functionality is occurring in different namespaces, eg dev-int-prod
      - could test things in dev and then migrate to other clusters just by changing namespace
- when you start to need them, start using them, usually for groups of people of 10 or more
- until then, using "default" is just fine

- can use "resource quotas" to limit resources each namespace can use

- not useful for different versions of things
- better for logical organizations, projects
  - eg, software dev, hardware, HR

RESOURCE LIMITS IN NAMESPACES
=============================
- namespaces can be assigned ResourceQuota objects
- each namespace should have at most one {but not required}
- this will limit the amount of usage allowed by the objects in that namespace
- you can limit, eg,
  - compute
  - storage
  - memory
  - how many objects exist

ResourceQuotas can be used for lots of things - refer to documentation, eg, providing access control to objects based upon namespaces

USEFUL COMMANDS FOR NAMESPACES
==============================
- similar to K8S commands for other types of objects
- use kubectl
  - kubectl create namespace <namespace_nm>
  - kubectl get namespace

PRACTICAL EXAMPLE
=================
- let's example a deployment that requests a certain amount of CPU 
  - 200m per container w/3 replicas for a total of 600m of CPU
  - in a namespace that only has 400m of cpu allocated using resource limits
- let's examine how k8s handles this
- our steps will include :
  - create the namespace
  - assign the namespace a 400m CPU resource limit
  - deploy tomcat (as we have in the past) into this new namespace but this time w/3 replicas requesting 200m CPU each
  - use kubectl to examine deployment status



 1393  kubectl get namespace
 1394  kubectl create namespace cpu-limited-tomcat
 1395* kubectl describe namespace
 1396  cat cpu-limits.yaml 
 1397  kubectl create -f cpu-limits.yaml --namespace cpu-limited-tomcat
 1398  kubectl describe namespace cpu-limited-tomcat
       kubectl get resourcequota --namespace=cpu-limited-tomcat
       cat tomcat-deployment.yaml 
       kubectl apply -f tomcat-deployment.yaml --namespace=cpu-limited-tomcat
       kubectl describe deployment tomcat-deployment --namespace=cpu-limited-tomcat
       - should fail because we didn't set up enough resources, and it doesn't

bobzawislak (master *) 04_Usage_and_Resource_Monitoring $ kubectl describe deployment tomcat-deployment --namespace=cpu-limited-tomcat

Name:                   tomcat-deployment
Namespace:              cpu-limited-tomcat
CreationTimestamp:      Sat, 02 Jun 2018 15:15:31 -0400
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision=1
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"apps/v1beta2","kind":"Deployment","metadata":{"annotations":{},"name":"tomcat-deployment","namespace":"cpu-limited-tomcat"},"spec":{"rep...
Selector:               app=tomcat
Replicas:               3 desired | 0 updated | 0 total | 0 available | 3 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=tomcat
  Containers:
   tomcat:
    Image:      tomcat:9.0
    Port:       8080/TCP
    Host Port:  0/TCP
    Requests:
      cpu:        200m
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type             Status  Reason
  ----             ------  ------
  Available        False   MinimumReplicasUnavailable
  ReplicaFailure   True    FailedCreate
  Progressing      False   ProgressDeadlineExceeded
OldReplicaSets:    <none>
NewReplicaSet:     tomcat-deployment-6759c57f78 (0/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set tomcat-deployment-6759c57f78 to 3


- it could have done 2 of the 3 but K8S FAILED IT IN IN ALL-OR-NOTHING FASHION

- look at the CONDITIONS and EVENTS areas of a DECRIBE-EMPLOYMENT output to get clues on success / failure conditions
  - eg, Available  False   MinimumReplicasUnavailable - indicates resources allocated to namespace was not enough to handle deployment desc



05 AUTO-SCALING
===============

- k8s provides a built-in facility called a HORIZONTAL POD AUTOSCALER {HPA} that adjusts the number of replicas of a pod to match observed average CPU utilization to a target specified by the user

- there are a variety of configurable options - quite a few - the key takeaway is to know that HPS will create new pods {or remove pods} from a replica to maintain average CPU utilization across all Pods to a level specified when you create your HPA - subject to conditions you specify 


CREATING AN HPA
===============
- kubectl autopsical provides the needed functions to create an HPA
- to create an autoscaler on our wordpress deployment that targets CPU utilization to an average of 50% per Pod, within the parameters of a minimum of 1 pod and up to a maximum of 10 pods
  - kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 --max=10

PRACTICAL EXAMPLE
=================
- artificially limit our WordPress Pod so we can stress it easily
  - kubectl apply -f wordpress-deployment.yaml  {in this chapter}
- add an HPA to enable auto-scaling on our WordPress installation to keep CPU at 50% average (or lower), with a minimum of 1 Pod and max of 5 Pods
  - kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 --max=5
- {in a separate terminal window}
  - simulate load using an infinite HTTP request loop from a worker Pod on our cluster to Wordpress to spike our CPU and see how HPA responds
    - kubectl run -i --tty load-generator --image=busybox /bin/sh
      hit enter at command prompt, then
      > while true;do wget -q -O- http://wordpress.default.svc.cluster.local;done
- check the HPA status immediately and after about a minute
  - kubectl get spa

- most modern machines can handle most current requests so in order to get this example to work, have to "modify" our running system so that it looks like it is running on hardware/resources that can be tested by this example

 1435  kubectl apply -f wordpress-deployment_just_requests_cpu_100m.yaml 
 1436  kubectl get deployments
 1437  kubectl describe deployments wordpress
 Containers:
   wordpress:
    Image:      wordpress:4.8-apache
    Port:       80/TCP
    Host Port:  0/TCP
    Requests:
      cpu:  100m     <== this now shows up as part of the deployment

kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 --max=5
  - deployment.apps "wordpress" autoscaled

kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 --max=10
kubectl apply -f ./wordpress-deployment.yaml
kubectl autoscale deployment wordpress --cpu-percent=50 --min=1 â€”max=5
{in another windows}
kubectl run -i --tty load-generator --image=busybox /bin/sh
while true; do wget -q -O- http://wordpress.default.svc.cluster.local; done
   - i kept getting "network unreachable" errors - don't think it was actually doing 
{back in same window as original set of commands, keep checking to see the load-generator cause a spike and increase in pods}
kubectl get hpa
   - if working properly - should see the TARGET and Pods numbers change
   - mine weren't - i was having issues with the load generator working
   - asked course about it


06 AUDITING
===========

Security and Auditing - IMPORTANT PRODUCTION SYSTEMS

What has gone on ?,
How it happened,
Where it happened, and
Who or what has caused an event to happen ?

IMPORTANT ASPECT : has to be free from access from people who can change the records it keeps

Records have to be accurate and TRUSTED

The purpose is to be able to investigate everything from simple innocent mistakes all  the way up to intentional criminal / destructive activity

Since K8S provides a framework that allows systems to perform any kind of activity - it also provides an auditing facility

Different TYPES exist
- Legacy auditing, which was SUNSET, and, 
- Advanced auditing, 
- Standard / default auditing

This course looks at advanced

WHAT's K8S AUDITING ?
=====================
- K8S auditing provides a security-relevant chronological set of records documenting the sequence of activities that have affected the system
- Can help answer what happened, why, by whom, on what, where, how it was initiated, and where it was going
- The kube-apiserver performs auditing according to the Audit Policy and sends records to the Audit Backend

The auditing process runs outside of kubectl and any of the pods running but monitors all of their activities

It is configured outside of the cluster

2 LARGE AREAS of CONCERN
- AUDIT POLICY
  - defines 'what' should be logged
  - defined in a yaml file like any other K8S object but since this is auditing, kubectl won't help you much here
    - auditing is defined on the kube-apiserver, it's an option that's passed to kube-apiserver when it starts, this varies by cluster
    - luckily, for minikube, we just pass a few options (like our Audit Policy) when we start minikube
  - nearly anything you can create or do can be audited in K8S
- AUDIT BACKEND
  - Audit Backends export audit events to external storage 
  - out of the box, kube-apiserver provides 2 backends
    - Logs - which writes events to disks
    - Webhooks - sends events to an external API/system

PRACTICAL EXAMPLE
=================
- let's set up a simple Audit Policy tools all requests regarding Metadata, this will let us see what Audit data looks like without overload
- we'll configure minikube to log it to a file on our file system
- our steps :
  - stop minikube (if running)
  - define an Audit Policy
  - start minikube with Audit Policy defined and appropriate options
    - won't start with audit-policy from course - gets start-up errors - posted a question

- when it works, minikube ssh onto the machine, navigate to the startup logs path and cat the audit logs
  - cat audit.logs | jq . 


07_EXERCISE
===========

see files in repo



08 HIGH AVAILABILITY
====================

Same adage for all software/hardware systems "The system is only as reliable as the stability of the system it is running on - if any piece fails, could mean catastrophe for the whole thing"


1 way to make a system more reliable - redundancy, replication & graceful handling of faults that occur in the system


KEY K8S ARCHITECTURE ITEMS
- Master 
  {runs the following 3 processes, at least}
  kube-apiserver
  kube-controller-manager
  kube-scheduler
- etcd
  distributed keystone that saves all cluster data
  not part of k8s - part of Linux OS
  fast scalable distributed datastore

RECOMMENDED to NOT RUN as the same machines that are running master


SETTING UP A RELIABLE SYSTEM
1. Creating reliable nodes that, together, will form a highly available master implementation in following steps
   - the actual Linux machines that the Master and etcd will run
   - Separate independent Linux machines that will run master processes
   - should run kubelet & monit
   - how many and where depends on the needs of your application and business
     - eg, if you're only running a business in 1 location - makes no sense to set up machines 1/2 around the world
       -do what makes sense business-wise, financially, practically, etc
2. Seeing up a redundant, reliable data storage layer with clustered etcd
   - etcd is a system that stores key and value pairs that is the data in a k8s cluster
   - it should run on every node that will be a master
   - consult the etcd documentation on the variety of options on how etcd can provide even deeper levels of redundancy if needed
     - etcd is not a standard k8s component, part of host os, part of system cfg is to use it
3. Starting replicated, load balanced API servers {kube-api}
   - kube-api should run on all nodes that will be a master
     - allows for command line kubectl interaction
   - kube-api should be behind a network load balancer, this will vary with your environment you are running 
     - load balancing is not a standard k8s component - has to be set up by hosting env and cfg'd to interact w/k8s
4. Setting up master-elected kube-scheduler and kube-controller-manager daemons
   - now that the above items are set up on reliable nodes, we have the pieces in place, but they aren't actually running
   - we need to ensure only 1 actor works on the data at a given time
   - each scheduler and controller manager will be started with a "--leader-elect" flag that will use a lease-lock API between themselves to ensure only 1 instance of each is running at a given time
5. Multiple worker nodes
   - 

KOPS - k8s tools that assist in performing each one of these

Up to this point - all of the work has been on a minikube - ie, master & worker nodes all on 1 machine

If something went wrong with that 1 machine - we're effed

In order to set up an HA system - can no longer do that - have to split up the components onto multiple machines, and even then if you just have the masters on 1 machine and something goes wrong there - you're effed again - so to really make it pretty reliable, we now have to make multiple masters on multiple machines, multiple etcds on multiple machines and multiple nodes on multiple machines

The higher number of multiple components running on multiple machines increases the chances that your system will be up and highly available - 1 "bad" side effect of this is that you'll have to have something in place to make sure all the machines are operating correctly and that bad/down machines get reported  and corrected promptly.

Another thing to consider - "multiple" machines also means different locations so that they won't be affected by the same storm, same loss of electricity, same fiber cut, same natural disaster yada yada yada  - best way is to separate geographically.

Some issues to consider when designing your k8s systems:
  - running 10 masters will usually cost 10 times as running 1 master
  - running multiple masters in multiple locations will need to take into account connectivity issues, data throughout, time lags, networking, etc




