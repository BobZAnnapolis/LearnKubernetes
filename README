UDEMY Learn Kubernetes From A DevOps Guru Notes
===============================================


Section 1
=========

course examples available at https://github.com/jleetutorial/kubernetes-demo

minikube - runs actually k8s code, best for testing out stuff on workstations

MINIKUBE
- all-in-one install of k8s
- takes all the distributed components of k8s and packages them into a single vm to run locally
- a few caveats :
  - mk does NOT support Cloud Provider specific features such as Load Balancers, Persistent Volumes
  - mk requires Virtualization - on Linux VirtualBox or VMWare Fusion

Terminology
K8S "deployments" are the high-level constructs that define an application
"pods" are instances of a container in a deployment
"services" are endpoints that expose ports to the outside world
You can delete, create, modify and retrieve information about any of these using the kubectl commands and local UI

Mac OS
- brew update
- brew install kubectl
  - needed to install Xcode command line tools
- brew install homebrew/cask/minikube
- brew cask install virtualbox

- minikube start
  kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
  kubectl expose deployment hello-minikube --type=NodePort
  kubectl get pod
  curl $(minikube service hello-minikube --url)
  kubectl delete deployment hello-minikube
  kubectl get pod
  minikube stop


What does a k8s app look like ?
- 'deployments' are the central metaphor for what we'd consider 'apps' or 'services'
- deployments are described as a collection of resources and references
- deployments take many forms based on the type of services being deployed
- typically described in YAML format

1st k8s app, tomcat
- deploy tomcat app server using official docker image
- tasks
  - define the deployment
    - most simple deployment is a single pod, no redundancy, no separation of services
    - a pod is an instance of a container
    - deployments can have any number of pods required to get the job done
  - expose its services
  - deploy it to our cluster

If a registry isn't specified - k8s will default to online docker hub

- minikube start
- define the deployment
  - kubectl apply -f kubernetes-demo/01_Introduction_to_Kubernetes/Your\ First\ k8s\ App/deployment.yaml
- map exposed service in container to outside world 
  - kubectl expose deployment tomcat-deployment --type=NodePort
- ask minikube what port it chose to expose
  - minikube service tomcat-deployment --url
- verify we could access the app at the given port
  - curl http://192.168.99.100:32047


KUBECTL
- basic command to interact w/any k8s cluster, local, remote, etc
- primary command line access tool
- kubectl get pods
  - list all running
- kubectl describe pods
  - detailed info on all
  - specify name to list just 1
  - lists LOTS of info

To have running containers communicate with outside world, have to expose a port 
  - kubectl expose 
    - exposes a port {UDP or TCP} for a given deployment, pod, or other resource
    - can specify a port or let k8s select one

kubectl port-forward
  - forwards 1 or more local ports to a pod
  - kubectl port-forward <pod name> [LOCAL_PORT:]REMOTE_PORT]

kubectl attach
  - attach to a pod to see its output
  - attaches to a process that is already running inside an existing container 

kubectl exec
  - execute a command inside a container
  - -i option will pass stdin to the container
  - -t option will specify stdin as TTY
  - eg, get to a bash shell to debug
    - kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 bash

kubectl label pods
  - updates labels on a pod for record-keeping, auditing purposes
  - eg, kubectl label pods tomcat-deployment-56ff5c79c5-vkrj9 healthy=false
    - "healthy=false" associated with this pod

kubectl run
  - instead of using a YAML deployment file
  - can also specify a command line image to run
  - runs the specified image on the cluster, specify name, port to expose, etc


KUBECTL REFERENCES & CHEAT SHEET
https://kubernetes.io/docs/user-guide/kubectl/v1.10
https://kubernetes.io/docs/user-guide/kubectl-cheatsheet

bash commands executed :
  543  xcode-select --install
  544  brew install kubectl
  545  kubectl
  551  kubectl version
  553  brew install homebrew/cask/minikube
  556  brew cask install virtualbox
  557  minikube start
  558  kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
  559  kubectl expose deployment hello-minikube --type=NodePort
  560  kubectl get pod
  561  curl $(minikube service hello-minikube --url)
  562  kubectl delete deployment hello-minikube
  563  kubectl get pod
  564  minikube stop
  565  history
  567  cat kubernetes-demo/01_Introduction_to_Kubernetes/Your\ First\ k8s\ App/deployment.yaml 
  569  minikube start
  570  kubectl apply -f kubernetes-demo/01_Introduction_to_Kubernetes/Your\ First\ k8s\ App/deployment.yaml 
  571  kubectl expose deployment tomcat-deployment --type=NodePort
  573  minikube service tomcat-deployment --url
  574  curl http://192.168.99.100:32047
  575  history
  577  kubectl get pods
  579  kubectl describe pods tomcat-deployment-56ff5c79c5-vkrj9
  580  kubectl get pods
  581* kubectl attach tomcat-deployment-56ff5c79c5-vkrj
  582  kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 bash
  587  kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 hostname
  588  kubectl exec -it tomcat-deployment-56ff5c79c5-vkrj9 bash
  589  kubectl label pods tomcat-deployment-56ff5c79c5-vkrj9 healthy=false
  590  kubectl get pods
  591  kubectl run hazelcast --image=hazelcast --port=5701
  592  kubectl describe pods
  593  kubectl version
  594  history


================================================================================
================================================================================


Section 2 Architecture Overview
===============================

Masters and Nodes

developer/operator/sysadmin/engineer interacts w/master(s)

Masters 
  - API Server
  - Scheduler
  - Controller Manager  (cmgr)

cmgr interacts w/k8s node(s)

nodes
  - kubelet
  - kube-proxy
  - 1 or more pods
  - Docker

users access the applications running on the pod(s)

A node is a 'worker machine' in k8s, a vm or a physical machine
Has the services necessary to run pods

Each node has a common set of services, eg, Docker, kubelet, cube-proxy
kubelet - monitors/runs pods
kube-proxy - glue on each node that makes sure network services on each node can be accessed

How should applications be designed to take advantage of scaling
stateful versus stateless
how state is handled can either enable or interfere with scaling, it most certainly dictates how you'll scale

stateful applications create/save application-/session-specific data locally - essentially lock apps to 1 machine - not a good idea if you want/need scaling
if that server dies - that state is lost - it is also the only 1 that knows what state that user's data is in

stateless applications update a central data store, eg, db, when necessary and any server can handle any request at any time to continue processing

K8S provides many ways to implement replication/scaling in your deployment
YAML file has a 'replicas' keyword -most popular
- define a replicaset
- deploy multiple bare pods
- define jobs
- daemonset


If your original deployment is still running and did NOT specify REPLICAS > 1, 
to prevent any downtime, you can issue a kubectl command
- kubectl scale --replicas=4 deployment/tomcat-deployment
- or if it's stopped, when you re-deploy - modify the replicas keyword in the YAML file


you can run the get pods or get deployments to see that there are now 4 replicas runnings
how does k8s decide which one gets a deployment ?

the only way pods can be exposed to the outside world is thru defined services

we previously ran the "kubectl expose deployment tomcat-deployment --type=NodePort" cmd when there was just 1 instance running

the "Load Balancer" service provides this function

the load balancer exposes just a single port but it decides which replica to send the request to
kubectl expose deployment tomcat-deployment --type=LoadBalancer --port=8080 --target-port=8080 --name tomcat-load-balancer
kubectl describe services tomcat-load-balancer
Name:                     tomcat-load-balancer
Namespace:                default
Labels:                   app=tomcat
Annotations:              <none>
Selector:                 app=tomcat
Type:                     LoadBalancer
IP:                       10.108.164.206
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  31552/TCP
Endpoints:                172.17.0.3:8080,172.17.0.6:8080,172.17.0.7:8080 + 1 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


DEPLOYMENTS
===========
high level object in k8s
define a desired state of an application

consist of pods

optionally, they can also include replicasets - which will be automagically managed by the deployment based upon its replica cfg

with deployment objects, you can :
- create a newdeployment
- update an existing {eg, set number of replicas}
- apply rolling updates to pods running on your cluster
- rollback to a previous version
- pause & resume

kubectl is your gateway to working w/deployments
useful commands
list deployments
  - kubectl get deployments
view status of deployment rollouts
  - kubectl rollout status deployment deployment_nm
set the image of a deployment
  - kubectl set image
    - look in the deployment.yaml file - in the container section - can change the image in any container using
      this command as long as we know the name, image version and deployment name
      eg, use get deployments to get deployment name, kubectl set image deployment/tomcat-deployment tomcat=tomcat=tomcat:9.0.1; do jubectl describe deployments afterward to see new image version
view the history of a rollout, including previous revisions
  - kubectl rollout history deployment tomcat-deployment to see list of mods
  - add --revision=# to get description of the change


LABELS & SELECTORS
==================

as the cluster grows, there will be an increasing number of pods, resources, services, etc - keeping track of everything can get cumbersome 

a method to keep things organized and to hols you and k8s identify resources to act upon

labels are key/value pairs that you can attach to objects like pods
  - they are for users to help describe meaningful and relevant info about an object
  - they do not affect the semantics of the core system
selectors are a way of expressing how to select objects based upon their labels
  - simple language to define what labels match and which ones do
  - you can specify if a label equals a given criteria or if it fits inside a set of criteria
    - equality-based
    - set-based

you can label nearly anything in the k8s world
  - deployments
  - services
  - nodes

eg, use labels to label a nice that it has SSD storage and then use a selector to tell the deployment that our app should only ever go onto a node with SSD storage



HEALTH CHECKING
===============

what happens when something in our cluster goes wrong ?

k8s has 'health checks'
- readiness problems
- liveness problem

readiness probes : determine when a pod is "ready", eg, after it has started to see when it's ready and has loaded what it needs to internally in the image and is ready to take requests from external services

liveness probes : determine when a pod is 'healthy' or 'unhealthy' after it has become ready

whether they are a readiness or a liveness probe they can use a variety of methods to ascertain a container's status
  - successful HTP or TCP request to the pod
  - successful command execution on the pod
probes are defined on the container in a Deployment or Pod specification

on our tomcat example :
- a readiness probe will check to make sure the Pod has started and is ready to begin taking requests
- a liveness probe on the tomcat deployment will help us ensure the containers continue to be able to accept and service requests without error in a reasonable amount of time

  657  cd Labels\ \&\ Selectors/
  658  lsa
  660  cat deployment.yaml 
  661  kubectl describe deployments
  662  cat deployment.yaml 
  663  kubectl get nodes
  664  kubectl label node minikube storageType=ssd
  665  kubectl get nodes
  666  kubectl describe node minikube
       - see storage=ssd in labels section
  667  cat deployment.yaml 
  668  kubectl apply -f deployment.yaml 
  669  cd..
  670  cd Health\ Checks/
  671  lsa
  674  cat deployment.yaml 
  675  kubectl apply -f ./deployment.yaml 
  676  kubectl describe deployment tomcat-deployment
       - see new lines with readiness and liveness probes running


WEB INTERFACE
=============

command line kubectl commands are the main interface
also has a web interface
called "Dashboard UI"
runs on k8s master(s)
accessible directly if you have direct network connectivity directly to your cluster/master(s) - unlikely in production envs

kubectl can create a proxy/tunnel if you do not "kubectl proxy"

provides a variety of views for nearly all of the command line kubectl commands

"minikube dashboard" command bring up UI


EXERCISE
========
get latest mongoldb replicates {4} up and running on our single node
1 way
  - 2 commands
    - kubectl run mongo-exercise-1 --image=mongo --port=27017
    - kubectl scale --replicas=4 deployment/mongo-exercise-1

Other ways
  - write a deployment file, use kubectl apply & kubectl expose
  - write a deployment file and a service file and use kubectl apply on both
  - use a K8S package Manager like helm to handle the work for you {advanced}
  - Use the Dashboard UI

  703  cd 02_Basic_and_Core_Concepts/
  705  cd Scaling\ \&\ Replication/
  707  cat deployment.yaml 
  708  cp deployment.yaml mongodb.yaml 
  710  vim mongodb.yaml 
  714  kubectl apply -f mongodb.yaml 
  778  kubectl expose deployment mongodb-deployment --type=NodePort
  748  kubectl get pods
  777  kubectl describe deployment mongodb-deployment
  780  kubectl get deployments
  780  kubectl get deployments

================================================================================
================================================================================

Section 3
=========

DNS & Service Discovery
=======================

DNS - Domain Name Service, cornerstone of the Internet
translates names into IP addresses
K8S has a build-in DNS service that is launched & cfg'd automatically
K8S cfgs kubelets to tell individual containers to use the DNS service's IP to resolve DNS names

Service Discovery : Modern software best practices consist of having completely separate deployment, scaling and configuration steps to build modern systems. 

How do we get them to communicate with each other in order to create a viable useful application in a containable way ? Service Discovery

Every service in your K8S cluster gets a DNS name
K8S has a specific and consistent nomenclature for deciding what this DNS name is
- general form is <my-service-name>.<my-namespace>.svc.cluster.local
But, you can use this well-known pattern to design loosely-coupled services or at least set "sane defaults"
- 1 could use environment variables to set the DNS name of other services your app may need
- or you could expect services at set DNS names
- or combine the 2 above and expect services at well-known DNS names and allow environment variables to override


a note on namespaces
- we'll discuss namespaces in depth soon in their own lecture, but for now, know the following
- k8s allows you to define "namespaces" - they allow you to separate your cluster into smaller logical clusters
  - by default, everything you do is in the "default" namespace (and everything we've done so far has been)
  - k8s DNS names will include this namespace in the assigned DNS name
- namespaces are helpful mostly for large clusters with many users across many teams & projects
  - using the default namespace is fine if this logical separation is not required 
  - eg of when needed, small homework assignment :no; MySQL instances for large corporations - have namespaces for the different departments would help create logical separations

HOW APP01 COULD REACH APP02 USING DNS

POD01
  - has a container
  - running Application_1
  - has a SERVICE: APP1 w/ip: 10.0.0.1

POD02
  - has a container
  - running Application_2
  - has a SERVICE: APP2 w/ip: 10.0.0.2

$ HOST APP1-SERVICE
APP1-SERVICE HAS ADDRESS 10.0.0.1
$ HOST APP2-SERVICE
APP2-SERVICE HAS ADDRESS 10.0.0.2
$ HOST APP2-SERVICE.DEFAULT
APP2-SERVICE.DEFAULT HAS ADDRESS 10.0.0.2
$ HOST APP2-SERVICE.DEFAULT.SVC.CLUSTER.LOCAL
APP2-SERVICE.DEFAULT.SVC.CLUSTER.LOCAL HAS ADDRESS 10.0.0.2

When using K8S - it has a built-in kube-dns 'process' that handles dns resolution automatically and is configured to use the default cfg'd namespaces so as long as the cluster abides by that naming convention - the built-in kube-dns will work

  910  kubectl get deployments
  911  kubectl delete deployment/mongodb-deployment
  913  kubectl delete deployment/tomcat-deployment
  914  kubectl get deployments
  915  kubectl get pods
  919  vim ./mysql-deployment.yaml 
  920  kubectl create -f ./mysql-deployment.yaml 
  921  kubectl get pods
  922  lsa
  923  vim wordpress-deployment.yaml 
  924  kubectl create -f ./wordpress-deployment.yaml 
  925  kubectl get pods
  933  kubectl get services wordpress
  934  minikube service wordpress --url
  - copy-paste-url to access wordpress site


VOLUMES
=======

Normally, unless otherwise configured - data in a Docker container is ephemeral, ie, once container is shut down - data vanishes.

This is essentially a stateless implementation - it is at the core of Kubernetes and containerization.

Downside - what do you do if you need persistent data ? That's where volumes come in.

Kubernetes provides directives that users cause to "attach" their physical volumes to their logical containers.

Volumes are like disks, but with a bit more
- volumes can be considered just a directory, with some data, which containers in a pod can access
- K8S supports multiple types of volumes that take care of how that data is stored, persisted, and made available
  - Support for a variety of cloud providers' block store products
  - Support for SAN-type hardware, file systems, etc
  - Support for local volumes {for testing/minikube only ! Not production}
- Certain types of Volumes can also provide sharing of files between pods babying mounted to multiple Pods simultaneously

Types of Volumes
- this is an incomplete list {see K8S documentation for complete list}
- Cloud Provider
  - Azure Disk & Azure File
  - AWS EBS - Elastic Block Store
  - Google Compute Engine Persistent Disk
- SAN/File System/Hardware
  - CephFS
  - Fibre Channel
  - GlusterFS
  - NFS
  - iSCSI
  - Local (For development/minikube only - not supported in production)


USING VOLUMES
=============
Pods can specify what volumes they need and where to mount them
  - Using the spec.volumes field(what volumes they need)
  - Using the spec.containers.volumeMounts field (where to mount them)

Processes in the containers then see a filesystem view of the data in that Volume

Using Volumes lets us separate stateless portions of our application (the code) from stateful data
  - The infrastructure can be scaled, maintained, and live separately from the data it works on/with
  - Also may ease portability, backup, recovery, and other management tasks in well-architected systems

We don't need to backup the Docker containers cuz they live inside a repository
But we may need to backup the data - separating the pieces this way, ie code in Docker registry, data in Volumes - allows us a variety of options to keep a system backed up

Separating them out eases the management of backing an app up

USING PERSISTENT VOLUMES
========================
PersistentVolumes ate Volumes designed to provide persistent disk-like functionality, ie, data survives reboots
Using then involves :
  - Provisioning a PersistentVolume (akin to creating/installing a disk in a virtual machine or hardware server)
  - Establishing a PersistentVolumeClaim (it is a request for storage by a user/Pod)
By examining available PersistentVolumes and demands from PersistentVolumeClaims by running Pods, K8S binds available volumes to Pods based on the options specified by the PersistentVolumeClaims to matching PersistentVolumes, eg, by name, storage size, storage class, etc asked for in the Claims


HOW CLAIMS & VOLUMES INTERACT
=============================
Administrator will define Volumes available for use, based upon available options
Developer/User will add VolumeClaims to their pod deployment yams files
When the app starts - the claims are sent to the K8S Master who matches claims to the available defined volumes and does the attaching to the Pods at runtime

Sometimes, if possible, the K8S Master will allocate new volumes under certain circumstances, eg, load balancing....if it can - ie, dynamic provisioning


CREATING A VOLUME
=================
PersistentVolumes are defined using a "PersistentVolume" definition that specifies their type, size, and how they can be accessed
Their type and access type is highly dependent on the underlying media 
- local disk
- network mount
- Cloud Block Storage Service
- Directory on the Host (testing only, not for production)

YAML files have a set of keywords to use based upon access type, eg local

bobzawislak (master *) 02_Volumes $ cat local-volumes.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
  labels:
    type: local
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data/pv-1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-2
  labels:
    type: local
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data/pv-2

Based upon the "kind" - access K8S documentation for all the other parms


CLAIMING A VOLUME
=================
Pods use PersistentVolumeClaims to request physical storage defined by PersistentVolumes
K8S uses the Claim to look for a PersistentVolume that satisfies the claim's requirements
- if it finds a match, it binds the claim to the volume
- if it cannot find a match (& automatic provisioning is NOT enabled), it results in an error


videos walked thru creating the local volumes, applying the new YAML files to the running WP site,
getting the URL to the WP site, adding a post - if saved correctly, indicates that the information
was saved into the new volumes

delete the wordpress deployment  and try to reload - browser can't - kubectl get deployments lists just the wordpress-mysql deployment

recreate the wordpress deployment using existing file - ok to ignore some of the errors cuz we're previously created a few of them

once the deployment exists - get URL {should be the same} and re-load

 1086  kubectl create -f local-volumes.yaml
 1087  kubectl apply -f mysql-deployment.yaml 
 1088  kubectl apply -f wordpress-deployment.yaml 
 1089  kubectl get pods
 1092  kubectl describe pod wordpress-5cddd8c7b-vf2s4
 1093  kubectl describe pod wordpress-mysql-5577fbc78-fnq7z
 1095  cat wordpress-deployment.yaml 
 1096  minikube service wordpress --url
       - create the wp site
       - login
       - create a post
       - get to home page and see the post - this shows data was saved in the persistent volume db
 1097  kubectl get deployments
 1098  kubectl delete deployment wordpress
       - this works - can no longer load the page
 1099  kubectl get deployments
 1100  kubectl create -f wordpress-deployment.yaml 
       - this says it works
       - but get a ContainerCfgError involving mysql-secrets
 1101  minikube service wordpress --url
       - never gets ready because of above

overall tho - can see how dividing up the functionality is an advantage 


SECRETS
=======

